---
title: "R lecture notes 2023-11-29"
author: "Adam Tallman"
date: "2023-11-24"
output:
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lattice)
library(Rling)
library(languageR)
library(nhstplot)
library(reshape)
elp.df <- read.csv("/Users/Adan Tallman/Desktop/ELP_full_length_frequency.csv")
senses <- read.csv("/Users/Adan Tallman/Desktop/winter_2016_senses_valence.csv")
data(ldt)
data(sharedref)
```

## Linear models

A deductive versus a statistical model can be seen by simulation.

```{r}
##Deductive model
x <- seq(from=1, to=20)
b <- 1
a <- 0
y <- a + b*x
plot(y~x)+lines(y,x)
```

Simulating a stochastic model would look like this.

```{r}
##Statistical model
set.seed(12345)
x <- seq(from=1, to=20)
b <- 1
a <- 0
e <- rnorm(m=0,sd=2, n=20)
y <- a + b*x + e
plot(y~x)+abline(y,x)
```

Those error terms are the ``residuals"` that we discussed last lecture.

```{r}
y_hat <- predict(lm(y~x))
residuals <- y - y_hat

error_residuals <- make.groups(e, residuals)

ggplot(error_residuals, aes(x=data, fill=which))+
  geom_density(alpha=0.3)
```

How do I know that my line is making predictions

```{r}
##Line through unrelated vectors
set.seed(12345)
x <- rnorm(20, 10, 4)
y <- rnorm(20, 5, 3)
plot(y~x)+abline(a=4, b=0.2)

```

How do we know when our line explains anything? Let's look at the ldt database. We can visualize the variance by using a straight line through the datapoints. The x-axis is just the order of elements in the vector. 

```{r}
data(ldt)
RT <- ldt$Mean_RT
plot(1:100,RT,ylim=c(500,1500),ylab="y",xlab="order",pch=21,bg="darkred")
abline(h=mean(RT),col="darkblue")
for(i in 1:100) lines(c(i,i),c(mean(RT),RT[i]),col="darkgreen")
```
You are calculating the overall distance the points are from the mean.

```{r}
var(RT)
```

```{r}
Length <- ldt$Length
plot(Length,RT,pch=21,bg="darkblue")
abline(lm(RT~Length),col="darkgreen")
fitted <- predict(lm(RT~Length))
lines(c(0,0),c(12,11.755556))
for (i in 1:100)
  lines (c(Length[i],Length[i]),c(RT[i],fitted[i]),col="darkred")
```
```{r}
sqrt(sum((RT-fitted)^2))
```
I will give an overview of how to read an lm() model below.

```{r}
model_1 <- lm(Mean_RT~Length, data=ldt)
summary(model_1)
```

## Analysis of Variance

```{r}
head(senses)
tail(senses)
```

How to imagine your null model? It would just be a model that tried to predict every VAL from the mean of all the values. You can visual this as a line going through the mean.

```{r}
senses_01 <- filter(senses, Modality =="Taste" | Modality =="Sound")
modality <- senses_01$Modality
Val <- senses_01$Val
plot(1:114,Val,ylim=c(4,7),ylab="Val",xlab="order",pch=21,bg="darkred")
abline(h=mean(Val),col="darkblue") 
for(i in 1:114) 
  lines(c(i,i),c(mean(Val),Val[i]),col="darkgreen")
```
The variances are distances from the mean visualized as lines in the plot above. The overall varianxes will be difference if you split the data into groups, which can be visualized as different lines.

```{r}

senses_01 <- filter(senses, Modality =="Taste" | Modality =="Sound")
modality <- senses_01$Modality
Val <- senses_01$Val
plot(1:114,Val,ylim=c(5,6.5),ylab="Val",xlab="order", pch=21,bg= as.numeric(as.factor(modality)))
#abline(h=mean(Val[modality=="Sight"]),col="darkgreen")
#abline(h=mean(Val[modality=="Smell"]), col="darkred")
abline(h=mean(Val[modality=="Sound"]), col="black")
#abline(h=mean(Val[modality=="Taste"]), col="cornflowerblue")
abline(h=mean(Val[modality=="Taste"]), col="red")
```

```{r}
senses_01 <- filter(senses, Modality =="Taste" | Modality =="Sound")
modality <- senses_01$Modality
Val <- senses_01$Val
plot(1:114,Val,ylim=c(5,6.5),ylab="Val",xlab="order", pch=21,bg= as.numeric(as.factor(modality)))
#abline(h=mean(Val[modality=="Sight"]),col="darkgreen")
#abline(h=mean(Val[modality=="Smell"]), col="darkred")
abline(h=mean(Val[modality=="Sound"]), col="black")
#abline(h=mean(Val[modality=="Taste"]), col="cornflowerblue")
abline(h=mean(Val[modality=="Taste"]), col="red")
index <- 1:length(Val)
for (i in 1:length(index)){
if (modality[i] == "Sound")
  lines(c(index[i],index[i]),c(mean(Val[modality=="Sound"]),Val
[i]))
else
  lines(c(index[i],index[i]),c(mean(Val[modality=="Taste"]),Val
[i]), col="red")
}
```

The difference can be formalized as the error sum of squares.


$$ SSE = \sum^k_{j=1} \sum(y - \overline{y}_j)^2 $$

Here's a way of calculating it using base R functions

```{r}
sound <- senses_01[senses_01$Modality=="Sound",]
taste <- senses_01[senses_01$Modality=="Taste",]
residuals_Sound <- sound$Val - mean(sound$Val)
residuals_Taste <- taste$Val - mean(taste$Val)
error_sum_of_squares <- sum(residuals_Sound^2) + sum(residuals_Taste^2)
error_sum_of_squares
```
The `analysis' part of the `Analysis of Variance' involves comparing this number to the total sum of squares.


```{r}
total_sum_of_squares <- sum((senses_01$Val - mean(senses_01$Val))^2)
```

The part of the variance that is explained by the different is called the ``treatment sum of squares', and that's just the the total sum of squares minus the error sum of squares.

```{r}
treatment_sum_of_squares <- total_sum_of_squares - error_sum_of_squares
```


Basically the F-statistic is as follows

$$F = \frac{\textrm{Variance explained}}{\textrm{Variance of error}} $$


You calculate the variance by dividing the sum of squares by their degrees of freedom.

```{r}
F_ratio <- treatment_sum_of_squares / (error_sum_of_squares/112)
```

```{r}
plotftest(f = 49.54, dfnum = 1, dfdenom = 112)
```


And there is a function in R that can do this.

```{r}
summary(aov(Val~Modality, data=senses_01))
```

## Chi-squared test

Let's take a look at some of the data from Matthew Dryer's paper on word order correlations.

```{r}
adpos <- matrix(c(107,12,7,70),ncol=2,byrow=TRUE)
rownames(adpos)<-c("PostP","Prep")
colnames(adpos)<-c("OV","VO")
adpos
```
```{r}
wordorder <- cbind(c(107, 7), c(12, 70))
rownames(wordorder) <- c("Postp", "Prep")
colnames(wordorder) <- c("OV", "VO")
wordorder <- rbind(wordorder, c(114,82))
wordorder <- cbind(wordorder, c(119,77,196))
rownames(wordorder) <- c("PostP", "Prep", "Column Total")
colnames(wordorder) <- c("OV", "VO", "Row total")
wordorder
```
Here's how we create the expected frequencies

```{r}
E <- cbind(c((114*119)/196, (114*77)/196), 
           c(82*119/196,(82*77)/196))
rownames(E) <- c("Postp", "Prep")
colnames(E) <- c("OV", "VO")
E
```

```{r}
E.df <- melt(E)
colnames(E.df)<-c("Adposition", "Verb.Object", "Expected.Frequency")
E.df$Observed.Frequency <- c(107,7,12,70)
E.df
```


$$ \chi^2 = \sum\frac{(Observed - Expected)^2}{Expected} $$

```{r}
E.df$oe <- ((E.df$Observed.Frequency - E.df$Expected.Frequency)^2) / E.df$Expected.Frequency
sum(E.df$oe)
```

```{r}
chisq.test(adpos)
```
