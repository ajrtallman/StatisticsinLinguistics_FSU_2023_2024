---
title: "R-lecture-notes-2023-12-13-multivariate-models-causation-interactions"
author: "Adam Tallman"
date: "2023-12-11"
output: word_document
---

```{r setup, include=TRUE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dagitty)
library(ggdag)
library(V8)
library(Rling)
library(AICcmodavg)
library(tidyverse)
library(gridExtra)
```

## Multivariate regression

- We have dealt with cases where there is one predictor
- The power of regression comes for multivariate regression
- This is the main tool that allows us to distinguish between cause and effect

```{r}
coord_dag <- list(
  x = c(X=1, Y=3),
  y = c(X=1, Y=1)
)

our_dag <- ggdag::dagify(Y~X,
                         coords = coord_dag)
ggdag::ggdag(our_dag)

```

```{r}
coord_dag <- list(
  x = c(X=1, Y=3, Z=2),
  y = c(X=1, Y=1, Z=3)
)

our_dag <- ggdag::dagify(Y~X,
                         X~Z,
                         Y~Z,
                         coords = coord_dag)
ggdag::ggdag(our_dag)
```

Let's simulate a confound which shows why its important to have more than one variable.

```{r}
set.seed(1234)
z <- rnorm(100, 10, 10)
b1 <- 2
b2 <- 3
a1 = 3
a2 = 4
y <- a1 + b1*z + rnorm(100, 0, 3)
x <- a2 + b2*z + rnorm(100, 0, 3)
d <- list(y,
          z,
          x)

summary(lm(y~x, data=d))
```
```{r}
summary(lm(y~x+z, data=d))
```

## Interactions

```{r}
data("sharedref")
head(sharedref)
```


```{r}
model1 <- lm(mod~age, data=sharedref)
anova(model1)
```
```{r}
ref <- aggregate(mod~age+cohort, data=sharedref, FUN = mean)
interaction.plot(ref$age, 
                 ref$cohort, 
                 ref$mod, 
                 xlab="age", 
                 ylab ="modulation",
                 lty = 1,
                 lwd = 2,
                 col = c("blue4", "red4"),
                 trace.label = "Cohort")
```

```{r}
model2 <- lm(mod~age*cohort, data=sharedref)
anova(model2)
```


```{r}
summary(model2)
```

## Model fitting and overfitting


```{r}
par(mfrow=c(2,2))
curve(4+2*x-0.1*x^2,0,10,col="red",ylab="y")
curve(4+2*x-0.2*x^2,0,10,col="red",ylab="y")
curve(12-4*x+0.3*x^2,0,10,col="red",ylab="y")
curve(4+0.5*x+0.1*x^2,0,10,col="red",ylab="y")
```

```{r}
par(mfrow=c(1,1))
data <- read.csv("/Users/User/Documents/GitHub/StatisticsinLinguistics_FSU_2023_2024/07_data/decay.csv", header=TRUE)
attach(data)
plot(time,amount,pch=21,col="blue",bg="green")
abline(lm(amount~time),col="red")
```

```{r}
model2 <- lm(amount~time)
model3 <- lm(amount~time+I(time^2))
```


```{r}
summary(model2)
```


```{r}
summary(model3)
```


```{r}
predict_model3 <- data.frame(amount_pred = predict(model3, data),
                            time= data$time)
p1 <- ggplot(data=data, aes(x=time, y =amount))+
  geom_point(color='blue')+
  geom_line(color='red', data=predict_model3, aes(x=time, y=amount_pred))+
  ggtitle("R^2 = 0.9014")
predict_model2 <- data.frame(amount_pred = predict(model2, data), time= data$time)

p2 <- ggplot(data=data, aes(x=time, y =amount))+
  geom_point(color='blue')+
  geom_line(color='red', data=predict_model2, aes(x=time, y=amount_pred))+
  ggtitle("R^2 = 0.7608")

grid.arrange(p2, p1, nrow = 1, ncol =2)
```


```{r}
model4 <- lm(amount~time+I(time^2)+I(time^3))
predict_model4 <- data.frame(amount_pred = predict(model4, data), time= data$time)

p3 <- ggplot(data=data, aes(x=time, y =amount))+
geom_point(color='blue')+
    geom_line(color='red', data=predict_model4, aes(x=time, y=amount_pred))+
    ggtitle("R^2 = 0.9055, model3")
model5 <- lm(amount~time+I(time^2)+I(time^3)+I(time^4))
predict_model5 <- data.frame(amount_pred = predict(model4, data), time= data$time)

p4 <- ggplot(data=data, aes(x=time, y =amount))+
    geom_point(color='blue')+
    geom_line(color='red', data=predict_model5, aes(x=time, y=amount_pred))+
    ggtitle("R^2 = 0.9069, model4")
grid.arrange(p3, p4, ncol=2)
```
## Akaike Information Criterion


```{r}
AIC(model2)
AIC(model3)
AIC(model4)
AIC(model5)
```

```{r}
entropytwo<-function(x,y)
{z<- 1-x-y
ifelse(z>0,-x*log2(x)-y*log2(y)-z*log2(z),0)
}
x<-(0:100)/100
y<-(0:100)/100
z<-outer(x,y,entropytwo)
persp(x, y, z, theta = 15, phi = 30, expand =
0.5, col = "darkred", xlab="p1", ylab="p2",
zlab="Entropy", main="H, n=3")
```

## Multivariate regression

```{r}
icon <- read.csv("/Users/User/Documents/GitHub/StatisticsinLinguistics_FSU_2023_2024/07_data/perry_winter_2017_iconicity.csv")
head(icon)
```

```{r}
p1 <- ggplot(icon, aes(x=Freq, y = Iconicity))+
  geom_point()+
  xlab("Frequency")
p2 <- ggplot(icon, aes(x=Freq, y = Syst))+
  geom_point()+
  xlab("Systematicity")
p3 <- ggplot(icon, aes(x=SER, y = Iconicity))+
  geom_point()+
  xlab("Sensory Experience Rating")
p4 <- ggplot(icon, aes(x= Conc, y = Iconicity))+
  geom_point()+
  xlab("Concreteness")
grid.arrange(p1, p2, p3, p4, nrow = 2, ncol =2)
```


```{r}
ggplot(icon, aes(x=POS, y = Iconicity))+
  geom_boxplot()+
  xlab("Part of Speech")+
  theme(axis.text.x = element_text(angle = 90))
```


```{r}
p1 <- qplot(icon$Freq)
icon$logFreq <- log(icon$Freq)
p2 <- qplot(icon$logFreq)
grid.arrange(p1, p2, ncol=2)
```
```{r}
model.saturated <- lm(Iconicity~logFreq+Syst+Conc+SER, data=icon)
summary(model.saturated)
```
```{r}
model2 <- lm(Iconicity~logFreq+Conc+SER, data=icon)
summary(model2)
```
```{r}
AIC(model.saturated)
AIC(model2)
```

